{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "celeb_face_diff_std8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4cHVyR7PUhw"
      },
      "source": [
        "# CS330: Unsupervised Feature-Learning via Meta-learning\n",
        "Test on CelebA dataset with labeled data. This can serve as a base line.\n",
        "dataGenerator here can be used with StyleGAN trained protonet for testing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIQdHpdxcnkE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "121fed91-864c-4a8b-d5b8-6bfc4fa69e00"
      },
      "source": [
        "# Load weight into StyleGAN model\n",
        "\n",
        "!git clone https://github.com/dnmarch/stylegan.git\n",
        "!gdown --id 1GLDZpU3N_fb-5kgVG-dTmNq0PxlE-iM3\n",
        "!gdown --id 1TKbv_bR6KZOPr_ms2yuz5s9Fk2HEl2MR\n",
        "from stylegan.DataGenerator import *\n",
        "from stylegan.stylegan import *\n",
        "from stylegan.ProtoNet import *\n",
        "model = StyleGAN_G()\n",
        "model.built = True\n",
        "model.load_weights('./stylegan_c.h5')\n",
        "weights = dict()\n",
        "for layer in model.layers:\n",
        "    print(layer.name)\n",
        "    weights[layer.name] = layer.get_weights()\n",
        "resolution=1024\n",
        "latent_size=512\n",
        "dlatent_size=512\n",
        "mapping_layers=8\n",
        "mapping_fmaps=512\n",
        "mapping_lrmul=0.01\n",
        "\n",
        "mapping = StyleGAN_G_mapping(latent_size, dlatent_size, mapping_layers, mapping_fmaps, mapping_lrmul)\n",
        "synthesis = StyleGAN_G_synthesis(dlatent_size, resolution)\n",
        "for name, weight in weights.items():\n",
        "    if \"mapping\" in name:\n",
        "        mapping.set_weights(weight)\n",
        "    else:\n",
        "        synthesis.set_weights(weight)\n",
        "\n",
        "###########################################################################################################\n",
        "###########################################################################################################\n",
        "###########################################################################################################\n",
        "###########################################################################################################\n",
        "###########################################################################################################\n",
        "import os\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "# Download the CelebAMask-HQ\n",
        "if not os.path.isdir('./CelebAMask-HQ'):\n",
        "    gdd.download_file_from_google_drive(file_id='1bOvIbpuuibn11VB7qyNjbuSsGh_VBGel',\n",
        "                                        dest_path='./CelebAMask-HQ.zip',\n",
        "                                        unzip=True)\n",
        "\n",
        "!pwd\n",
        "#assert os.path.isdir('./CelebAMask-HQ')\n",
        "\n",
        "# Download img_align_celeba\n",
        "if not os.path.isdir('./img_align_celeba'):\n",
        "    gdd.download_file_from_google_drive(file_id='1CkLRsaegBjdQ73cM-G9gK8AjMCsHeCaH',\n",
        "                                        dest_path='./img_align_celeba.zip',\n",
        "                                        unzip=True)\n",
        "\n",
        "\n",
        "#assert os.path.isdir('./img_align_celeba')\n",
        "\n",
        "# Download the annotation and other files\n",
        "if not os.path.isdir('./celebA_anno'):\n",
        "    gdd.download_file_from_google_drive(file_id='1Q6u5E2xiuqp8ov8PhCsa8RiG1VTzhrvX',\n",
        "                                        dest_path='./celebA_anno.zip',\n",
        "                                        unzip=True)\n",
        "\n",
        "\n",
        "#assert os.path.isdir('./Anno')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'stylegan'...\n",
            "remote: Enumerating objects: 80, done.\u001b[K\n",
            "remote: Counting objects: 100% (80/80), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 80 (delta 42), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (80/80), done.\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1GLDZpU3N_fb-5kgVG-dTmNq0PxlE-iM3\n",
            "To: /content/stylegan.h5\n",
            "116MB [00:00, 280MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1TKbv_bR6KZOPr_ms2yuz5s9Fk2HEl2MR\n",
            "To: /content/stylegan_c.h5\n",
            "116MB [00:00, 173MB/s]\n",
            "WARNING:tensorflow:From /content/stylegan/stylegan.py:73: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "Model created.\n",
            "G_mapping\n",
            "G_synthesis\n",
            "Downloading 1bOvIbpuuibn11VB7qyNjbuSsGh_VBGel into ./CelebAMask-HQ.zip... Done.\n",
            "Unzipping...Done.\n",
            "/content\n",
            "Downloading 1CkLRsaegBjdQ73cM-G9gK8AjMCsHeCaH into ./img_align_celeba.zip... Done.\n",
            "Unzipping...Done.\n",
            "Downloading 1Q6u5E2xiuqp8ov8PhCsa8RiG1VTzhrvX into ./celebA_anno.zip... Done.\n",
            "Unzipping...Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e18hqC5tmOqG"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRVvIL1u1W7_"
      },
      "source": [
        "!mv Anno celebA_anno"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQN89Iz35zmE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caefdf10-8801-465a-93f6-812e1cf5d291"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "celebA_anno\t CelebAMask-HQ.zip     sample_data    stylegan.h5\n",
            "celebA_anno.zip  img_align_celeba      stylegan\n",
            "CelebAMask-HQ\t img_align_celeba.zip  stylegan_c.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuSr5LqL1NVI"
      },
      "source": [
        "\"\"\"Data Generator\"\"\"\n",
        "import os\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import random\n",
        "from scipy import misc\n",
        "import imageio\n",
        "import cv2\n",
        "\n",
        "\n",
        "######################## Directory ########################\n",
        "WORK_DIR = \"/content\"\n",
        "celebA_dir = os.path.join(WORK_DIR, 'celebA_anno')\n",
        "celebA_HQ_dir = os.path.join(WORK_DIR, 'CelebAMask-HQ')\n",
        "img_align_dir = os.path.join(WORK_DIR, 'img_align_celeba')\n",
        "###########################################################\n",
        "\n",
        "def read_img(img_path, resize='cut'):\n",
        "    \"\"\"cv2 reads BGR, Pic is RGB, reorder the the 2 axies\"\"\"\n",
        "    img = imageio.imread(img_path)\n",
        "    assert img is not None\n",
        "    img = img_resize(img, resize)\n",
        "    img = img.astype(np.float32) / 255\n",
        "    img = 1 - img\n",
        "    return img\n",
        "\n",
        "def img_resize(img, how='cut'):\n",
        "    \"\"\"The CelebA img is (218, 178, 3), we can either resize to (256, 256) \n",
        "    or cut 0 dim to [178, 178, 3]\"\"\"\n",
        "    how = how.lower()\n",
        "    assert how in ['cut', 'resize']\n",
        "    if how=='cut':\n",
        "        img = img[20:198, :, :]\n",
        "        assert img.shape == (178, 178, 3)\n",
        "    if how=='resize':\n",
        "        img = cv2.resize(img, (256,256))\n",
        "    return img\n",
        "\n",
        "def get_labels_df():\n",
        "    id_celebA = pd.read_table(os.path.join(celebA_dir, \"identity_CelebA.txt\"), \n",
        "                    delim_whitespace=True, \n",
        "                    names=['file', 'idx']\n",
        "                    )\n",
        "    mapping = pd.read_table(os.path.join(celebA_HQ_dir, \"CelebA-HQ-to-CelebA-mapping.txt\"), \n",
        "                    delim_whitespace=True, \n",
        "                    names=['hq_idx','idx', 'file'], skiprows=1\n",
        "                    )                \n",
        "\n",
        "    # Get a list of HQ images that also exist in CelebA by checking the idx\n",
        "    # Left join and see file is not Nan\n",
        "    mapping = mapping.merge(id_celebA, how='left', on=['idx'], suffixes=('_hd', ''))\n",
        "    mapping = mapping[~mapping.file.isna()]\n",
        "\n",
        "    imgs = mapping.groupby('idx').apply(lambda x: x.shape[0]).\\\n",
        "            reset_index().rename(columns={0: 'counter'})\n",
        "\n",
        "    imgs = imgs.merge(mapping, how='left', on=['idx'])\n",
        "    # Find all idx with more than 10 images set to dataset ~ to omniglot\n",
        "    multi_face = imgs[imgs.counter>= 10].copy()\n",
        "    multi_face.sort_values('idx', inplace=True)\n",
        "\n",
        "    assert len(multi_face.file.unique()) == len(multi_face)\n",
        "\n",
        "    # Create Labels by re-map idx\n",
        "    label_dict = {}\n",
        "    for i, idx in enumerate(multi_face.idx.unique()):\n",
        "        label_dict[idx] = i\n",
        "\n",
        "    multi_face['label'] = [label_dict[k] for k in multi_face.idx.values] \n",
        "    # aligned images are in png format\n",
        "    # multi_face.file = [x.replace('jpg', 'png') for x in multi_face.file]\n",
        "    return multi_face\n",
        "\n",
        "def get_images(df, hq_idx, labels, n_samples=None, shuffle=True):\n",
        "    \"\"\"\n",
        "    Takes a set of character folders and labels and returns paths to image files\n",
        "    paired with labels.    \n",
        "    \"\"\"\n",
        "    if n_samples is not None:\n",
        "        sampler = lambda x: random.sample(x, n_samples)\n",
        "    else:\n",
        "        sampler = lambda x: x\n",
        "    images_labels = [(i, os.path.join(img_align_dir, image))\n",
        "                    for i, hd_i in zip(labels, hq_idx)\n",
        "                    for image in sampler(list(df[df.hq_idx==hd_i]['file'].values))\n",
        "    ]\n",
        "    if shuffle:\n",
        "        random.shuffle(images_labels)\n",
        "    return images_labels\n",
        "\n",
        "class DataLoader(object):\n",
        "  \"\"\"\n",
        "  Data Generator capable of generating batches of CelebA data.\n",
        "  Generated image in [B, N, K, h, w, c]\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_classes, num_samples_per_class, num_meta_test_classes, num_meta_test_samples_per_class, df):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      num_classes: Number of classes for classification (N-way)\n",
        "      num_samples_per_class: num samples to generate per class in one batch\n",
        "      num_meta_test_classes: Number of classes for classification (N-way) at meta-test time\n",
        "      num_meta_test_samples_per_class: num samples to generate per class in one batch at meta-test time\n",
        "      batch_size: size of meta batch size (e.g. number of functions)\n",
        "    \"\"\"\n",
        "    self.num_samples_per_class = num_samples_per_class\n",
        "    self.num_classes = num_classes\n",
        "    self.num_meta_test_samples_per_class = num_meta_test_samples_per_class\n",
        "    self.num_meta_test_classes = num_meta_test_classes\n",
        "    self.df = get_labels_df() # main df contains [idx, counter, hq_idx, file_hd, file, label]\n",
        "\n",
        "    WORK_DIR = \"/Users/zhejianpeng/Google Drive/GaTech/AI_Cert/CS330_Multitask_MetaLearning/Final_Project\"\n",
        "    celebA_dir = os.path.join(WORK_DIR, 'celebA_anno')\n",
        "    celebA_HQ_dir = os.path.join(WORK_DIR, 'CelebAMask-HQ')\n",
        "    img_align_dir = os.path.join(WORK_DIR, 'img_align_celeba')\n",
        "    \n",
        "    self.img_size = (178, 178, 3) # or (256, 256, 3)\n",
        "\n",
        "    self.resize_method = 'cut' # self.img_size \n",
        "    self.dim_output = self.num_classes\n",
        "\n",
        "    # Construct Meta-Train, Meta-Test, Meta-Val\n",
        "    face_idx = list(self.df.hq_idx.unique()) # Each hd_id map to multiple images of that face\n",
        "    random.seed(123)\n",
        "    np.random.seed(123)\n",
        "    random.shuffle(face_idx)\n",
        "    num_val = 100\n",
        "    num_train = 850\n",
        "    self.metatrain_face_idx = face_idx[:num_train]\n",
        "    self.metaval_face_idx = face_idx[num_train: num_train+num_val]\n",
        "    self.metatest_face_idx = face_idx[num_train+num_val:]   \n",
        "    print(f\"{num_train} Train, {num_val} Val {len(self.metatest_face_idx)} Test\")\n",
        "\n",
        "  def sample_batch(self, batch_type, batch_size, shuffle=True, swap=False):\n",
        "    \"\"\"\n",
        "    Samples a batch for training, validation, or testing\n",
        "    Args:\n",
        "      batch_type: meta_train/meta_val/meta_test\n",
        "      shuffle: randomly shuffle classes or not\n",
        "      swap: swap number of classes (N) and number of samples per class (K) or not\n",
        "    Returns:\n",
        "      A a tuple of (1) Image batch and (2) Label batch where\n",
        "      image batch has shape [B, N, K, h*w*c] and label batch has shape [B, N, K, N] if swap is False\n",
        "      where B is batch size, K is number of samples per class, N is number of classes\n",
        "    \"\"\"\n",
        "    if batch_type == \"meta_train\":\n",
        "        face_idx = self.metatrain_face_idx\n",
        "        num_classes = self.num_classes\n",
        "        num_samples_per_class = self.num_samples_per_class\n",
        "    elif batch_type == \"meta_val\":\n",
        "        face_idx = self.metaval_face_idx\n",
        "        num_classes = self.num_classes\n",
        "        num_samples_per_class = self.num_samples_per_class\n",
        "    else:\n",
        "        face_idx = self.metatest_face_idx\n",
        "        num_classes = self.num_meta_test_classes\n",
        "        num_samples_per_class = self.num_meta_test_samples_per_class\n",
        "    all_image_batches, all_label_batches = [], []\n",
        "    for i in range(batch_size):\n",
        "        sampled_character_folders = random.sample(\n",
        "            face_idx, num_classes)\n",
        "        labels_and_images = get_images(self.df, face_idx, range(\n",
        "            num_classes), n_samples=num_samples_per_class, shuffle=False)\n",
        "        labels = [li[0] for li in labels_and_images]\n",
        "        images = [read_img(\n",
        "            li[1], self.resize_method) for li in labels_and_images]\n",
        "        images = np.stack(images)\n",
        "        labels = np.array(labels).astype(np.int32)\n",
        "        labels = np.reshape(\n",
        "            labels, (num_classes, num_samples_per_class))\n",
        "        labels = np.eye(num_classes, dtype=np.float32)[labels]\n",
        "        images = np.reshape(\n",
        "            images, [num_classes, num_samples_per_class, -1])\n",
        "\n",
        "        batch = np.concatenate([labels, images], 2)\n",
        "        if shuffle:\n",
        "            for p in range(num_samples_per_class):\n",
        "                np.random.shuffle(batch[:, p])\n",
        "\n",
        "        labels = batch[:, :, :num_classes]\n",
        "        images = batch[:, :, num_classes:]\n",
        "\n",
        "        if swap:\n",
        "            labels = np.swapaxes(labels, 0, 1)\n",
        "            images = np.swapaxes(images, 0, 1)\n",
        "\n",
        "        # Since we flattend the images previously, we want to shape it back\n",
        "        # images = np.reshape(\n",
        "        #     images, [num_classes, \n",
        "        #              num_samples_per_class,\n",
        "        #              self.img_size[0],\n",
        "        #              self.img_size[1],\n",
        "        #              self.img_size[2]]\n",
        "        #             )\n",
        "\n",
        "        all_image_batches.append(images)\n",
        "        all_label_batches.append(labels)\n",
        "    all_image_batches = np.stack(all_image_batches)\n",
        "    all_label_batches = np.stack(all_label_batches)\n",
        "    return all_image_batches, all_label_batches\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAoytNKU6CDO"
      },
      "source": [
        "# df = get_labels_df()\n",
        "# # get_images(df, [5212, 9575, 27681, 6170, 13821], range(5), 5, False) \n",
        "# data_gen = DataGenerator(5, 6, 5, 8, df)\n",
        "# batch = data_gen.sample_batch('meta_train', batch_size=1, shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVMpeuBI6eoO"
      },
      "source": [
        "\"\"\" Utility functions. \"\"\"\n",
        "## NOTE: You do not need to modify this block but you will need to use it.\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "## Loss utilities\n",
        "def cross_entropy_loss(pred, label, k_shot):\n",
        "  \"\"\"pred, label, k_shot\"\"\"\n",
        "  # return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=tf.stop_gradient(label)) / k_shot)\n",
        "  return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=tf.stop_gradient(label)))\n",
        "\n",
        "\n",
        "def accuracy(labels, predictions):\n",
        "  \"\"\"labels, predictions\"\"\"\n",
        "  return tf.reduce_mean(tf.cast(tf.equal(labels, predictions), dtype=tf.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlpM5-T76Lud",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b9697ffd-dc6d-4dae-a174-081a9427b894"
      },
      "source": [
        "\n",
        "def run_protonet(n_way=20, k_shot=1, n_query=5, n_meta_test_way=20, k_meta_test_shot=5, n_meta_test_query=5):\n",
        "    \n",
        "    df = get_labels_df()\n",
        "    n_epochs = 200\n",
        "    n_episodes = 10\n",
        "\n",
        "    im_width, im_height, channels = 178, 178, 3\n",
        "    num_filters = 32\n",
        "    latent_dim = 512\n",
        "    num_conv_layers = 5\n",
        "    n_meta_test_episodes = 1000\n",
        "\n",
        "    model = ProtoNet([num_filters]*num_conv_layers, latent_dim)\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    # call DataGenerator with k_shot+n_query samples per class\n",
        "    # n_way=5, k_shot=1, n_query=5, n_meta_test_way=5, k_meta_test_shot=4, n_meta_test_query=4\n",
        "\n",
        "    # load real data from celebA dataset\n",
        "    data_loader = DataLoader(n_way, k_shot+n_query, n_meta_test_way, k_meta_test_shot+n_meta_test_query, df)\n",
        "    # synthetic data\n",
        "    resolution_low = 16\n",
        "    resolution_high = 64\n",
        "    num_std = 0.15\n",
        "    noise_std = 0.001\n",
        "\n",
        "    data_generator = DataGenerator(mapping, synthesis, K=k_shot + n_query, N=n_meta_test_way, resolution_start=resolution_low, resolution_end=resolution_high*2)\n",
        "\n",
        "    plot_val_acc = []\n",
        "    plot_test_acc = []\n",
        "    plot_train_acc = []\n",
        "    for ep in range(n_epochs):\n",
        "        tr_acc = []\n",
        "        for epi in range(n_episodes):\n",
        "        #############################\n",
        "        #### YOUR CODE GOES HERE ####\n",
        "            images, labels = data_generator.sample_batch(1, k_shot + n_query, n_meta_test_way, num_std=num_std, noise_std=noise_std, shuffle=False, swap=False, h=im_height, w=im_width)\n",
        "                #images, labels = data_generator.sample_batch(\"meta_train\", 1, shuffle=False)\n",
        "            input_tr = images[:, :, :k_shot]\n",
        "            input_ts = images[:, :, k_shot:]\n",
        "            label_tr = labels[:, :, :k_shot]\n",
        "            label_ts = labels[:, :, k_shot:]\n",
        "\n",
        "\n",
        "            support = np.reshape(input_tr, [n_way, k_shot, im_width, im_height, channels])\n",
        "            query = np.reshape(input_ts, [n_way, n_query, im_width, im_height, channels])\n",
        "            labels = np.reshape(label_ts, [n_way, n_query, n_way])\n",
        "            #############################\n",
        "            ls, ac = proto_net_train_step(model, optimizer, x=support, q=query, labels_ph=labels)\n",
        "            tr_acc.append(ac)\n",
        "            if (epi+1) % 10 == 0:\n",
        "            #############################\n",
        "            #### YOUR CODE GOES HERE ####\n",
        "\n",
        "            # sample a batch of validation data and partition into\n",
        "            # support and query sets\n",
        "                i, l = data_loader.sample_batch('meta_val', batch_size=1, shuffle=False) # inputs, labels\n",
        "                support, query = i[0, :, :-n_query, :], i[0, :, -n_query:, :]\n",
        "                labels = l[0, :, -n_query:, :]\n",
        "\n",
        "                # Randomly shuffle query set and labels\n",
        "                query = tf.reshape(query, [n_way*n_query, -1])\n",
        "                labels = tf.reshape(labels, [n_way*n_query, -1])\n",
        "                indices = tf.range(start=0, limit=n_way*n_query, dtype=tf.int32)\n",
        "                shuffled_indices = tf.random.shuffle(indices)\n",
        "                labels = tf.gather(labels, shuffled_indices)# , axis=0)\n",
        "                query = tf.gather(query, shuffled_indices)#, axis=0)\n",
        "                labels = tf.reshape(labels, [n_way, n_query, n_way])\n",
        "\n",
        "                support = tf.reshape(support, [n_way, k_shot, im_width, im_height, channels])\n",
        "                query = tf.reshape(query, [n_way, n_query, im_width, im_height, channels])\n",
        "\n",
        "                #############################\n",
        "                val_ls, val_ac = proto_net_eval(model, x=support, q=query, labels_ph=labels)\n",
        "\n",
        "            if (epi+1) % 20 == 0:\n",
        "                print('[epoch {}/{}, episode {}/{}] => meta-training loss: {:.5f}, meta-training acc: {:.5f}, meta-val loss: {:.5f}, meta-val acc: {:.5f}'.format(ep+1,\n",
        "                                                                          n_epochs,\n",
        "                                                                          epi+1,\n",
        "                                                                          n_episodes,\n",
        "                                                                          ls,\n",
        "                                                                          ac,\n",
        "                                                                          val_ls,\n",
        "                                                                          val_ac))\n",
        "                plot_val_acc.append(val_ac)\n",
        "        plot_train_acc.append(np.mean(tr_acc))\n",
        "        print('Training acc: {} at epoch {}, now testing...'.format(tr_acc[-1], ep))\n",
        "        meta_test_accuracies = []\n",
        "\n",
        "        for epi in range(n_meta_test_episodes):\n",
        "            #############################\n",
        "            #### YOUR CODE GOES HERE ####\n",
        "\n",
        "            # sample a batch of test data and partition into\n",
        "            # support and query sets\n",
        "            i, l = data_loader.sample_batch('test', batch_size=1, shuffle=False) # inputs, labels\n",
        "            support, query = i[0, :, :-n_meta_test_query, :], i[0, :, -n_meta_test_query:, :]\n",
        "            labels = l[0, :, -n_meta_test_query:, :]\n",
        "\n",
        "            # Randomly shuffle query set and labels\n",
        "            query = tf.reshape(query, [n_meta_test_way*n_meta_test_query, -1])\n",
        "            labels = tf.reshape(labels, [n_meta_test_way*n_meta_test_query, -1])\n",
        "            indices = tf.range(start=0, limit=n_meta_test_way*n_meta_test_query, dtype=tf.int32)\n",
        "            shuffled_indices = tf.random.shuffle(indices)\n",
        "            labels = tf.gather(labels, shuffled_indices)\n",
        "            query = tf.gather(query, shuffled_indices)\n",
        "            labels = tf.reshape(labels, [n_meta_test_way, n_meta_test_query, n_meta_test_way])\n",
        "\n",
        "            support = tf.reshape(support, [n_meta_test_way, k_meta_test_shot, im_width, im_height, channels])\n",
        "            query = tf.reshape(query, [n_meta_test_way, n_meta_test_query, im_width, im_height, channels])\n",
        "            #############################\n",
        "            ls, ac = proto_net_eval(model, x=support, q=query, labels_ph=labels)\n",
        "            meta_test_accuracies.append(ac)\n",
        "            if (epi+1) % 50 == 0:\n",
        "                pass\n",
        "                # print('[meta-test episode {}/{}] => loss: {:.5f}, acc: {:.5f}'.format(epi+1, n_meta_test_episodes, ls, ac))\n",
        "        avg_acc = np.mean(meta_test_accuracies)\n",
        "        stds = np.std(meta_test_accuracies)\n",
        "        plot_test_acc.append(avg_acc)\n",
        "        print('Average Meta-Test Accuracy: {:.5f}, Meta-Test Accuracy Std: {:.5f}'.format(avg_acc, stds))\n",
        "    return plot_val_acc, plot_test_acc, plot_train_acc\n",
        "\n",
        "\n",
        "def plot_acc(val_acc, name, save=False):\n",
        "    import matplotlib.pyplot as plt\n",
        "    title = f'{name}_Val_Acc'\n",
        "    plt.plot(val_acc)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Accuracy')\n",
        "    if save:\n",
        "        plt.savefig(f\"{title}.png\")\n",
        "    plt.show()\n",
        "    plt.clf()\n",
        "\n",
        "# val_acc = run_protonet('./omniglot_resized/', n_way=20, k_shot=1, n_query=5, n_meta_test_way=20,\n",
        "#                         k_meta_test_shot=5, n_meta_test_query=5)\n",
        "# plot_acc(val_acc, name='protonet_n20k1', save=True)\n",
        "\n",
        "\n",
        "# Prob. 3\n",
        "n_way = 3\n",
        "val_acc, test_acc, train_acc = run_protonet(n_way=n_way, k_shot=1, n_query=5, n_meta_test_way=n_way,\n",
        "                        k_meta_test_shot=6, n_meta_test_query=5)\n",
        "\n",
        "print(test_acc)\n",
        "print(train_acc)\n",
        "np.save(\"test\", test_acc)\n",
        "np.save(\"train\", train_acc)\n",
        "\n",
        "plot_acc(val_acc, name='protonet_n5k1', save=True)\n",
        "\n",
        "\n",
        "\n",
        "# Prob. 3\n",
        "#val_acc = run_protonet(n_way=5, k_shot=1, n_query=5, n_meta_test_way=5, \n",
        "#                        k_meta_test_shot=4, n_meta_test_query=4)\n",
        "              \n",
        "#plot_acc(val_acc, name='protonet_n5k1', save=True)\n",
        "\n",
        "# val_acc = run_protonet('./omniglot_resized/', n_way=30, k_shot=1, n_query=5, n_meta_test_way=30, \n",
        "#                         k_meta_test_shot=5, n_meta_test_query=5)\n",
        "# plot_acc(val_acc, name='protonet_n30k1', save=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "850 Train, 100 Val 324 Test\n",
            "4 12\n",
            "Training acc: 0.6666666865348816 at epoch 0, now testing...\n",
            "Average Meta-Test Accuracy: 0.54547, Meta-Test Accuracy Std: 0.13201\n",
            "Training acc: 0.5333333611488342 at epoch 1, now testing...\n",
            "Average Meta-Test Accuracy: 0.59033, Meta-Test Accuracy Std: 0.11365\n",
            "Training acc: 0.5333333611488342 at epoch 2, now testing...\n",
            "Average Meta-Test Accuracy: 0.65333, Meta-Test Accuracy Std: 0.10482\n",
            "Training acc: 0.20000000298023224 at epoch 3, now testing...\n",
            "Average Meta-Test Accuracy: 0.74367, Meta-Test Accuracy Std: 0.10015\n",
            "Training acc: 0.5333333611488342 at epoch 4, now testing...\n",
            "Average Meta-Test Accuracy: 0.64573, Meta-Test Accuracy Std: 0.10451\n",
            "Training acc: 0.6000000238418579 at epoch 5, now testing...\n",
            "Average Meta-Test Accuracy: 0.65860, Meta-Test Accuracy Std: 0.09987\n",
            "warning: standard deviation set to be too high\n",
            "minimum distance: 0.0213241558521986, noise level: 0.023583539575338364\n",
            "Training acc: 0.5333333611488342 at epoch 6, now testing...\n",
            "Average Meta-Test Accuracy: 0.67553, Meta-Test Accuracy Std: 0.10835\n",
            "Training acc: 0.9333333373069763 at epoch 7, now testing...\n",
            "Average Meta-Test Accuracy: 0.71127, Meta-Test Accuracy Std: 0.09728\n",
            "Training acc: 0.46666666865348816 at epoch 8, now testing...\n",
            "Average Meta-Test Accuracy: 0.74487, Meta-Test Accuracy Std: 0.10047\n",
            "Training acc: 0.5333333611488342 at epoch 9, now testing...\n",
            "Average Meta-Test Accuracy: 0.75867, Meta-Test Accuracy Std: 0.10249\n",
            "Training acc: 0.8666666746139526 at epoch 10, now testing...\n",
            "Average Meta-Test Accuracy: 0.74987, Meta-Test Accuracy Std: 0.09648\n",
            "Training acc: 0.8666666746139526 at epoch 11, now testing...\n",
            "Average Meta-Test Accuracy: 0.72760, Meta-Test Accuracy Std: 0.09799\n",
            "Training acc: 0.800000011920929 at epoch 12, now testing...\n",
            "Average Meta-Test Accuracy: 0.72620, Meta-Test Accuracy Std: 0.09521\n",
            "Training acc: 0.3333333432674408 at epoch 13, now testing...\n",
            "Average Meta-Test Accuracy: 0.57420, Meta-Test Accuracy Std: 0.12069\n",
            "Training acc: 0.5333333611488342 at epoch 14, now testing...\n",
            "Average Meta-Test Accuracy: 0.61353, Meta-Test Accuracy Std: 0.11360\n",
            "Training acc: 0.9333333373069763 at epoch 15, now testing...\n",
            "Average Meta-Test Accuracy: 0.66780, Meta-Test Accuracy Std: 0.11205\n",
            "Training acc: 0.800000011920929 at epoch 16, now testing...\n",
            "Average Meta-Test Accuracy: 0.72613, Meta-Test Accuracy Std: 0.10440\n",
            "warning: standard deviation set to be too high\n",
            "minimum distance: 0.02220674231648445, noise level: 0.023583539575338364\n",
            "warning: standard deviation set to be too high\n",
            "minimum distance: 0.020434023812413216, noise level: 0.023583539575338364\n",
            "Training acc: 0.5333333611488342 at epoch 17, now testing...\n",
            "Average Meta-Test Accuracy: 0.75080, Meta-Test Accuracy Std: 0.10794\n",
            "Training acc: 0.6666666865348816 at epoch 18, now testing...\n",
            "Average Meta-Test Accuracy: 0.72307, Meta-Test Accuracy Std: 0.11060\n",
            "Training acc: 0.800000011920929 at epoch 19, now testing...\n",
            "Average Meta-Test Accuracy: 0.73973, Meta-Test Accuracy Std: 0.10479\n",
            "Training acc: 0.5333333611488342 at epoch 20, now testing...\n",
            "Average Meta-Test Accuracy: 0.71787, Meta-Test Accuracy Std: 0.09938\n",
            "Training acc: 0.7333333492279053 at epoch 21, now testing...\n",
            "Average Meta-Test Accuracy: 0.74140, Meta-Test Accuracy Std: 0.09947\n",
            "Training acc: 0.7333333492279053 at epoch 22, now testing...\n",
            "Average Meta-Test Accuracy: 0.73507, Meta-Test Accuracy Std: 0.09873\n",
            "Training acc: 0.7333333492279053 at epoch 23, now testing...\n",
            "Average Meta-Test Accuracy: 0.72207, Meta-Test Accuracy Std: 0.10212\n",
            "Training acc: 0.6000000238418579 at epoch 24, now testing...\n",
            "Average Meta-Test Accuracy: 0.67940, Meta-Test Accuracy Std: 0.10655\n",
            "Training acc: 0.8666666746139526 at epoch 25, now testing...\n",
            "Average Meta-Test Accuracy: 0.67273, Meta-Test Accuracy Std: 0.10718\n",
            "Training acc: 0.9333333373069763 at epoch 26, now testing...\n",
            "Average Meta-Test Accuracy: 0.65993, Meta-Test Accuracy Std: 0.10559\n",
            "Training acc: 0.9333333373069763 at epoch 27, now testing...\n",
            "Average Meta-Test Accuracy: 0.65133, Meta-Test Accuracy Std: 0.10989\n",
            "Training acc: 0.8666666746139526 at epoch 28, now testing...\n",
            "Average Meta-Test Accuracy: 0.62893, Meta-Test Accuracy Std: 0.11051\n",
            "Training acc: 0.6666666865348816 at epoch 29, now testing...\n",
            "Average Meta-Test Accuracy: 0.65420, Meta-Test Accuracy Std: 0.10629\n",
            "Training acc: 0.7333333492279053 at epoch 30, now testing...\n",
            "Average Meta-Test Accuracy: 0.63627, Meta-Test Accuracy Std: 0.11255\n",
            "Training acc: 0.46666666865348816 at epoch 31, now testing...\n",
            "Average Meta-Test Accuracy: 0.69147, Meta-Test Accuracy Std: 0.10314\n",
            "Training acc: 0.800000011920929 at epoch 32, now testing...\n",
            "Average Meta-Test Accuracy: 0.72427, Meta-Test Accuracy Std: 0.10493\n",
            "Training acc: 0.800000011920929 at epoch 33, now testing...\n",
            "Average Meta-Test Accuracy: 0.68100, Meta-Test Accuracy Std: 0.10547\n",
            "Training acc: 0.46666666865348816 at epoch 34, now testing...\n",
            "Average Meta-Test Accuracy: 0.68627, Meta-Test Accuracy Std: 0.10586\n",
            "Training acc: 1.0 at epoch 35, now testing...\n",
            "Average Meta-Test Accuracy: 0.68333, Meta-Test Accuracy Std: 0.10728\n",
            "Training acc: 0.6666666865348816 at epoch 36, now testing...\n",
            "Average Meta-Test Accuracy: 0.66860, Meta-Test Accuracy Std: 0.10926\n",
            "Training acc: 0.8666666746139526 at epoch 37, now testing...\n",
            "Average Meta-Test Accuracy: 0.66960, Meta-Test Accuracy Std: 0.10742\n",
            "Training acc: 0.8666666746139526 at epoch 38, now testing...\n",
            "Average Meta-Test Accuracy: 0.68680, Meta-Test Accuracy Std: 0.11724\n",
            "Training acc: 0.8666666746139526 at epoch 39, now testing...\n",
            "Average Meta-Test Accuracy: 0.71173, Meta-Test Accuracy Std: 0.10589\n",
            "Training acc: 0.5333333611488342 at epoch 40, now testing...\n",
            "Average Meta-Test Accuracy: 0.68933, Meta-Test Accuracy Std: 0.11064\n",
            "Training acc: 0.8666666746139526 at epoch 41, now testing...\n",
            "Average Meta-Test Accuracy: 0.69553, Meta-Test Accuracy Std: 0.11372\n",
            "Training acc: 0.7333333492279053 at epoch 42, now testing...\n",
            "Average Meta-Test Accuracy: 0.64267, Meta-Test Accuracy Std: 0.12141\n",
            "Training acc: 0.9333333373069763 at epoch 43, now testing...\n",
            "Average Meta-Test Accuracy: 0.67273, Meta-Test Accuracy Std: 0.11025\n",
            "Training acc: 0.800000011920929 at epoch 44, now testing...\n",
            "Average Meta-Test Accuracy: 0.67513, Meta-Test Accuracy Std: 0.11153\n",
            "Training acc: 0.9333333373069763 at epoch 45, now testing...\n",
            "Average Meta-Test Accuracy: 0.73587, Meta-Test Accuracy Std: 0.09556\n",
            "Training acc: 0.6666666865348816 at epoch 46, now testing...\n",
            "Average Meta-Test Accuracy: 0.74987, Meta-Test Accuracy Std: 0.10186\n",
            "Training acc: 0.6666666865348816 at epoch 47, now testing...\n",
            "Average Meta-Test Accuracy: 0.78380, Meta-Test Accuracy Std: 0.09821\n",
            "Training acc: 0.6666666865348816 at epoch 48, now testing...\n",
            "Average Meta-Test Accuracy: 0.71680, Meta-Test Accuracy Std: 0.10504\n",
            "Training acc: 0.9333333373069763 at epoch 49, now testing...\n",
            "Average Meta-Test Accuracy: 0.70293, Meta-Test Accuracy Std: 0.10053\n",
            "Training acc: 0.9333333373069763 at epoch 50, now testing...\n",
            "Average Meta-Test Accuracy: 0.67447, Meta-Test Accuracy Std: 0.10998\n",
            "Training acc: 0.800000011920929 at epoch 51, now testing...\n",
            "Average Meta-Test Accuracy: 0.65413, Meta-Test Accuracy Std: 0.11498\n",
            "Training acc: 0.9333333373069763 at epoch 52, now testing...\n",
            "Average Meta-Test Accuracy: 0.63180, Meta-Test Accuracy Std: 0.12781\n",
            "Training acc: 0.8666666746139526 at epoch 53, now testing...\n",
            "Average Meta-Test Accuracy: 0.64580, Meta-Test Accuracy Std: 0.12970\n",
            "Training acc: 1.0 at epoch 54, now testing...\n",
            "Average Meta-Test Accuracy: 0.65447, Meta-Test Accuracy Std: 0.11630\n",
            "Training acc: 0.8666666746139526 at epoch 55, now testing...\n",
            "Average Meta-Test Accuracy: 0.69793, Meta-Test Accuracy Std: 0.11193\n",
            "Training acc: 1.0 at epoch 56, now testing...\n",
            "Average Meta-Test Accuracy: 0.72280, Meta-Test Accuracy Std: 0.11177\n",
            "Training acc: 0.8666666746139526 at epoch 57, now testing...\n",
            "Average Meta-Test Accuracy: 0.67060, Meta-Test Accuracy Std: 0.11921\n",
            "Training acc: 0.7333333492279053 at epoch 58, now testing...\n",
            "Average Meta-Test Accuracy: 0.67980, Meta-Test Accuracy Std: 0.11156\n",
            "Training acc: 0.9333333373069763 at epoch 59, now testing...\n",
            "Average Meta-Test Accuracy: 0.72267, Meta-Test Accuracy Std: 0.10709\n",
            "Training acc: 0.6000000238418579 at epoch 60, now testing...\n",
            "Average Meta-Test Accuracy: 0.74773, Meta-Test Accuracy Std: 0.10135\n",
            "warning: standard deviation set to be too high\n",
            "minimum distance: 0.021809402853250504, noise level: 0.023583539575338364\n",
            "Training acc: 0.800000011920929 at epoch 61, now testing...\n",
            "Average Meta-Test Accuracy: 0.69613, Meta-Test Accuracy Std: 0.10821\n",
            "Training acc: 0.6000000238418579 at epoch 62, now testing...\n",
            "Average Meta-Test Accuracy: 0.69540, Meta-Test Accuracy Std: 0.11371\n",
            "Training acc: 0.8666666746139526 at epoch 63, now testing...\n",
            "Average Meta-Test Accuracy: 0.63753, Meta-Test Accuracy Std: 0.11970\n",
            "Training acc: 0.9333333373069763 at epoch 64, now testing...\n",
            "Average Meta-Test Accuracy: 0.75967, Meta-Test Accuracy Std: 0.09944\n",
            "Training acc: 0.8666666746139526 at epoch 65, now testing...\n",
            "Average Meta-Test Accuracy: 0.78640, Meta-Test Accuracy Std: 0.09831\n",
            "Training acc: 0.800000011920929 at epoch 66, now testing...\n",
            "Average Meta-Test Accuracy: 0.80267, Meta-Test Accuracy Std: 0.09424\n",
            "Training acc: 0.7333333492279053 at epoch 67, now testing...\n",
            "Average Meta-Test Accuracy: 0.76473, Meta-Test Accuracy Std: 0.10543\n",
            "Training acc: 0.800000011920929 at epoch 68, now testing...\n",
            "Average Meta-Test Accuracy: 0.74273, Meta-Test Accuracy Std: 0.11390\n",
            "Training acc: 0.8666666746139526 at epoch 69, now testing...\n",
            "Average Meta-Test Accuracy: 0.75007, Meta-Test Accuracy Std: 0.10469\n",
            "warning: standard deviation set to be too high\n",
            "minimum distance: 0.02207028679549694, noise level: 0.023583539575338364\n",
            "Training acc: 0.9333333373069763 at epoch 70, now testing...\n",
            "Average Meta-Test Accuracy: 0.70813, Meta-Test Accuracy Std: 0.10606\n",
            "Training acc: 1.0 at epoch 71, now testing...\n",
            "Average Meta-Test Accuracy: 0.78460, Meta-Test Accuracy Std: 0.09375\n",
            "Training acc: 0.9333333373069763 at epoch 72, now testing...\n",
            "Average Meta-Test Accuracy: 0.81207, Meta-Test Accuracy Std: 0.09166\n",
            "Training acc: 0.800000011920929 at epoch 73, now testing...\n",
            "Average Meta-Test Accuracy: 0.79280, Meta-Test Accuracy Std: 0.09803\n",
            "Training acc: 0.7333333492279053 at epoch 74, now testing...\n",
            "Average Meta-Test Accuracy: 0.71247, Meta-Test Accuracy Std: 0.11110\n",
            "Training acc: 0.6000000238418579 at epoch 75, now testing...\n",
            "Average Meta-Test Accuracy: 0.72647, Meta-Test Accuracy Std: 0.10596\n",
            "warning: standard deviation set to be too high\n",
            "minimum distance: 0.02264293283224106, noise level: 0.023583539575338364\n",
            "warning: standard deviation set to be too high\n",
            "minimum distance: 0.021597208455204964, noise level: 0.023583539575338364\n",
            "warning: standard deviation set to be too high\n",
            "minimum distance: 0.023524869233369827, noise level: 0.023583539575338364\n",
            "Training acc: 1.0 at epoch 76, now testing...\n",
            "Average Meta-Test Accuracy: 0.73113, Meta-Test Accuracy Std: 0.10840\n",
            "Training acc: 1.0 at epoch 77, now testing...\n",
            "Average Meta-Test Accuracy: 0.70807, Meta-Test Accuracy Std: 0.10892\n",
            "Training acc: 0.9333333373069763 at epoch 78, now testing...\n",
            "Average Meta-Test Accuracy: 0.69700, Meta-Test Accuracy Std: 0.10566\n",
            "Training acc: 0.800000011920929 at epoch 79, now testing...\n",
            "Average Meta-Test Accuracy: 0.73173, Meta-Test Accuracy Std: 0.10690\n",
            "warning: standard deviation set to be too high\n",
            "minimum distance: 0.02329070121049881, noise level: 0.023583539575338364\n",
            "Training acc: 0.8666666746139526 at epoch 80, now testing...\n",
            "Average Meta-Test Accuracy: 0.69893, Meta-Test Accuracy Std: 0.11703\n",
            "Training acc: 0.6000000238418579 at epoch 81, now testing...\n",
            "Average Meta-Test Accuracy: 0.67187, Meta-Test Accuracy Std: 0.12532\n",
            "Training acc: 1.0 at epoch 82, now testing...\n",
            "Average Meta-Test Accuracy: 0.72593, Meta-Test Accuracy Std: 0.11045\n",
            "Training acc: 1.0 at epoch 83, now testing...\n",
            "Average Meta-Test Accuracy: 0.69593, Meta-Test Accuracy Std: 0.11393\n",
            "Training acc: 0.800000011920929 at epoch 84, now testing...\n",
            "Average Meta-Test Accuracy: 0.70300, Meta-Test Accuracy Std: 0.10811\n",
            "Training acc: 1.0 at epoch 85, now testing...\n",
            "Average Meta-Test Accuracy: 0.70387, Meta-Test Accuracy Std: 0.10944\n",
            "Training acc: 0.7333333492279053 at epoch 86, now testing...\n",
            "Average Meta-Test Accuracy: 0.67833, Meta-Test Accuracy Std: 0.10642\n",
            "Training acc: 0.9333333373069763 at epoch 87, now testing...\n",
            "Average Meta-Test Accuracy: 0.68127, Meta-Test Accuracy Std: 0.10304\n",
            "Training acc: 0.8666666746139526 at epoch 88, now testing...\n",
            "Average Meta-Test Accuracy: 0.65313, Meta-Test Accuracy Std: 0.10645\n",
            "Training acc: 0.8666666746139526 at epoch 89, now testing...\n",
            "Average Meta-Test Accuracy: 0.63867, Meta-Test Accuracy Std: 0.11876\n",
            "warning: standard deviation set to be too high\n",
            "minimum distance: 0.022095296531915665, noise level: 0.023583539575338364\n",
            "Training acc: 0.8666666746139526 at epoch 90, now testing...\n",
            "Average Meta-Test Accuracy: 0.64827, Meta-Test Accuracy Std: 0.11278\n",
            "Training acc: 1.0 at epoch 91, now testing...\n",
            "Average Meta-Test Accuracy: 0.69640, Meta-Test Accuracy Std: 0.11181\n",
            "Training acc: 0.800000011920929 at epoch 92, now testing...\n",
            "Average Meta-Test Accuracy: 0.70560, Meta-Test Accuracy Std: 0.11728\n",
            "Training acc: 0.9333333373069763 at epoch 93, now testing...\n",
            "Average Meta-Test Accuracy: 0.71000, Meta-Test Accuracy Std: 0.11123\n",
            "Training acc: 1.0 at epoch 94, now testing...\n",
            "Average Meta-Test Accuracy: 0.74093, Meta-Test Accuracy Std: 0.11205\n",
            "Training acc: 1.0 at epoch 95, now testing...\n",
            "Average Meta-Test Accuracy: 0.68820, Meta-Test Accuracy Std: 0.10639\n",
            "Training acc: 0.8666666746139526 at epoch 96, now testing...\n",
            "Average Meta-Test Accuracy: 0.68373, Meta-Test Accuracy Std: 0.12082\n",
            "Training acc: 0.5333333611488342 at epoch 97, now testing...\n",
            "Average Meta-Test Accuracy: 0.69140, Meta-Test Accuracy Std: 0.11906\n",
            "Training acc: 0.8666666746139526 at epoch 98, now testing...\n",
            "Average Meta-Test Accuracy: 0.69167, Meta-Test Accuracy Std: 0.11451\n",
            "Training acc: 0.8666666746139526 at epoch 99, now testing...\n",
            "Average Meta-Test Accuracy: 0.67467, Meta-Test Accuracy Std: 0.11684\n",
            "Training acc: 0.9333333373069763 at epoch 100, now testing...\n",
            "Average Meta-Test Accuracy: 0.69247, Meta-Test Accuracy Std: 0.12113\n",
            "warning: standard deviation set to be too high\n",
            "minimum distance: 0.023368285968899727, noise level: 0.023583539575338364\n",
            "warning: standard deviation set to be too high\n",
            "minimum distance: 0.024263257160782814, noise level: 0.023583539575338364\n",
            "warning: standard deviation set to be too high\n",
            "minimum distance: 0.021688273176550865, noise level: 0.023583539575338364\n",
            "Training acc: 1.0 at epoch 101, now testing...\n",
            "Average Meta-Test Accuracy: 0.71513, Meta-Test Accuracy Std: 0.10759\n",
            "Training acc: 0.9333333373069763 at epoch 102, now testing...\n",
            "Average Meta-Test Accuracy: 0.72347, Meta-Test Accuracy Std: 0.11357\n",
            "Training acc: 0.7333333492279053 at epoch 103, now testing...\n",
            "Average Meta-Test Accuracy: 0.73927, Meta-Test Accuracy Std: 0.10416\n",
            "Training acc: 0.9333333373069763 at epoch 104, now testing...\n",
            "Average Meta-Test Accuracy: 0.73213, Meta-Test Accuracy Std: 0.10439\n",
            "Training acc: 1.0 at epoch 105, now testing...\n",
            "Average Meta-Test Accuracy: 0.70700, Meta-Test Accuracy Std: 0.10602\n",
            "Training acc: 1.0 at epoch 106, now testing...\n",
            "Average Meta-Test Accuracy: 0.70420, Meta-Test Accuracy Std: 0.11047\n",
            "Training acc: 0.6666666865348816 at epoch 107, now testing...\n",
            "Average Meta-Test Accuracy: 0.73807, Meta-Test Accuracy Std: 0.10285\n",
            "Training acc: 0.9333333373069763 at epoch 108, now testing...\n",
            "Average Meta-Test Accuracy: 0.74240, Meta-Test Accuracy Std: 0.10353\n",
            "Training acc: 0.800000011920929 at epoch 109, now testing...\n",
            "Average Meta-Test Accuracy: 0.73960, Meta-Test Accuracy Std: 0.10744\n",
            "Training acc: 0.6000000238418579 at epoch 110, now testing...\n",
            "Average Meta-Test Accuracy: 0.71913, Meta-Test Accuracy Std: 0.10344\n",
            "Training acc: 0.7333333492279053 at epoch 111, now testing...\n",
            "Average Meta-Test Accuracy: 0.69813, Meta-Test Accuracy Std: 0.10993\n",
            "Training acc: 1.0 at epoch 112, now testing...\n",
            "Average Meta-Test Accuracy: 0.71880, Meta-Test Accuracy Std: 0.11267\n",
            "Training acc: 1.0 at epoch 113, now testing...\n",
            "Average Meta-Test Accuracy: 0.74060, Meta-Test Accuracy Std: 0.10686\n",
            "Training acc: 0.9333333373069763 at epoch 114, now testing...\n",
            "Average Meta-Test Accuracy: 0.74233, Meta-Test Accuracy Std: 0.10415\n",
            "Training acc: 0.9333333373069763 at epoch 115, now testing...\n",
            "Average Meta-Test Accuracy: 0.74373, Meta-Test Accuracy Std: 0.10175\n",
            "Training acc: 0.6666666865348816 at epoch 116, now testing...\n",
            "Average Meta-Test Accuracy: 0.75633, Meta-Test Accuracy Std: 0.10930\n",
            "Training acc: 0.6666666865348816 at epoch 117, now testing...\n",
            "Average Meta-Test Accuracy: 0.70853, Meta-Test Accuracy Std: 0.11063\n",
            "Training acc: 0.8666666746139526 at epoch 118, now testing...\n",
            "Average Meta-Test Accuracy: 0.71953, Meta-Test Accuracy Std: 0.11003\n",
            "Training acc: 0.9333333373069763 at epoch 119, now testing...\n",
            "Average Meta-Test Accuracy: 0.72227, Meta-Test Accuracy Std: 0.11180\n",
            "Training acc: 0.8666666746139526 at epoch 120, now testing...\n",
            "Average Meta-Test Accuracy: 0.67373, Meta-Test Accuracy Std: 0.11248\n",
            "Training acc: 0.8666666746139526 at epoch 121, now testing...\n",
            "Average Meta-Test Accuracy: 0.66560, Meta-Test Accuracy Std: 0.11254\n",
            "Training acc: 0.4000000059604645 at epoch 122, now testing...\n",
            "Average Meta-Test Accuracy: 0.68573, Meta-Test Accuracy Std: 0.11345\n",
            "Training acc: 1.0 at epoch 123, now testing...\n",
            "Average Meta-Test Accuracy: 0.67600, Meta-Test Accuracy Std: 0.10812\n",
            "Training acc: 0.8666666746139526 at epoch 124, now testing...\n",
            "Average Meta-Test Accuracy: 0.67507, Meta-Test Accuracy Std: 0.11112\n",
            "Training acc: 0.9333333373069763 at epoch 125, now testing...\n",
            "Average Meta-Test Accuracy: 0.70253, Meta-Test Accuracy Std: 0.10792\n",
            "Training acc: 0.46666666865348816 at epoch 126, now testing...\n",
            "Average Meta-Test Accuracy: 0.74173, Meta-Test Accuracy Std: 0.10224\n",
            "Training acc: 0.9333333373069763 at epoch 127, now testing...\n",
            "Average Meta-Test Accuracy: 0.75047, Meta-Test Accuracy Std: 0.09454\n",
            "Training acc: 0.7333333492279053 at epoch 128, now testing...\n",
            "Average Meta-Test Accuracy: 0.72640, Meta-Test Accuracy Std: 0.10585\n",
            "Training acc: 0.8666666746139526 at epoch 129, now testing...\n",
            "Average Meta-Test Accuracy: 0.70660, Meta-Test Accuracy Std: 0.11140\n",
            "Training acc: 0.9333333373069763 at epoch 130, now testing...\n",
            "Average Meta-Test Accuracy: 0.68480, Meta-Test Accuracy Std: 0.10999\n",
            "Training acc: 0.8666666746139526 at epoch 131, now testing...\n",
            "Average Meta-Test Accuracy: 0.67273, Meta-Test Accuracy Std: 0.11319\n",
            "Training acc: 0.800000011920929 at epoch 132, now testing...\n",
            "Average Meta-Test Accuracy: 0.65480, Meta-Test Accuracy Std: 0.11803\n",
            "Training acc: 0.8666666746139526 at epoch 133, now testing...\n",
            "Average Meta-Test Accuracy: 0.67740, Meta-Test Accuracy Std: 0.12204\n",
            "Training acc: 0.9333333373069763 at epoch 134, now testing...\n",
            "Average Meta-Test Accuracy: 0.68520, Meta-Test Accuracy Std: 0.11732\n",
            "Training acc: 0.8666666746139526 at epoch 135, now testing...\n",
            "Average Meta-Test Accuracy: 0.70527, Meta-Test Accuracy Std: 0.11071\n",
            "Training acc: 0.9333333373069763 at epoch 136, now testing...\n",
            "Average Meta-Test Accuracy: 0.73687, Meta-Test Accuracy Std: 0.10272\n",
            "Training acc: 0.800000011920929 at epoch 137, now testing...\n",
            "Average Meta-Test Accuracy: 0.70193, Meta-Test Accuracy Std: 0.11363\n",
            "Training acc: 1.0 at epoch 138, now testing...\n",
            "Average Meta-Test Accuracy: 0.68067, Meta-Test Accuracy Std: 0.11858\n",
            "Training acc: 0.6000000238418579 at epoch 139, now testing...\n",
            "Average Meta-Test Accuracy: 0.67093, Meta-Test Accuracy Std: 0.11501\n",
            "Training acc: 0.6000000238418579 at epoch 140, now testing...\n",
            "Average Meta-Test Accuracy: 0.65367, Meta-Test Accuracy Std: 0.12142\n",
            "Training acc: 0.5333333611488342 at epoch 141, now testing...\n",
            "Average Meta-Test Accuracy: 0.69367, Meta-Test Accuracy Std: 0.10614\n",
            "Training acc: 0.6000000238418579 at epoch 142, now testing...\n",
            "Average Meta-Test Accuracy: 0.69987, Meta-Test Accuracy Std: 0.10853\n",
            "Training acc: 0.9333333373069763 at epoch 143, now testing...\n",
            "Average Meta-Test Accuracy: 0.71267, Meta-Test Accuracy Std: 0.10675\n",
            "Training acc: 0.8666666746139526 at epoch 144, now testing...\n",
            "Average Meta-Test Accuracy: 0.68627, Meta-Test Accuracy Std: 0.11638\n",
            "Training acc: 0.800000011920929 at epoch 145, now testing...\n",
            "Average Meta-Test Accuracy: 0.65447, Meta-Test Accuracy Std: 0.10798\n",
            "Training acc: 0.800000011920929 at epoch 146, now testing...\n",
            "Average Meta-Test Accuracy: 0.68420, Meta-Test Accuracy Std: 0.11337\n",
            "Training acc: 1.0 at epoch 147, now testing...\n",
            "Average Meta-Test Accuracy: 0.69040, Meta-Test Accuracy Std: 0.11026\n",
            "Training acc: 0.9333333373069763 at epoch 148, now testing...\n",
            "Average Meta-Test Accuracy: 0.68880, Meta-Test Accuracy Std: 0.11669\n",
            "Training acc: 0.9333333373069763 at epoch 149, now testing...\n",
            "Average Meta-Test Accuracy: 0.69580, Meta-Test Accuracy Std: 0.11263\n",
            "Training acc: 0.800000011920929 at epoch 150, now testing...\n",
            "Average Meta-Test Accuracy: 0.68940, Meta-Test Accuracy Std: 0.11656\n",
            "Training acc: 0.5333333611488342 at epoch 151, now testing...\n",
            "Average Meta-Test Accuracy: 0.71987, Meta-Test Accuracy Std: 0.10135\n",
            "warning: standard deviation set to be too high\n",
            "minimum distance: 0.02247329242527485, noise level: 0.023583539575338364\n",
            "Training acc: 0.9333333373069763 at epoch 152, now testing...\n",
            "Average Meta-Test Accuracy: 0.76300, Meta-Test Accuracy Std: 0.10209\n",
            "Training acc: 1.0 at epoch 153, now testing...\n",
            "Average Meta-Test Accuracy: 0.76900, Meta-Test Accuracy Std: 0.09831\n",
            "Training acc: 0.7333333492279053 at epoch 154, now testing...\n",
            "Average Meta-Test Accuracy: 0.76647, Meta-Test Accuracy Std: 0.09848\n",
            "Training acc: 1.0 at epoch 155, now testing...\n",
            "Average Meta-Test Accuracy: 0.69747, Meta-Test Accuracy Std: 0.10854\n",
            "Training acc: 1.0 at epoch 156, now testing...\n",
            "Average Meta-Test Accuracy: 0.69940, Meta-Test Accuracy Std: 0.10409\n",
            "Training acc: 0.46666666865348816 at epoch 157, now testing...\n",
            "Average Meta-Test Accuracy: 0.70673, Meta-Test Accuracy Std: 0.11191\n",
            "Training acc: 0.9333333373069763 at epoch 158, now testing...\n",
            "Average Meta-Test Accuracy: 0.69493, Meta-Test Accuracy Std: 0.11164\n",
            "Training acc: 1.0 at epoch 159, now testing...\n",
            "Average Meta-Test Accuracy: 0.70060, Meta-Test Accuracy Std: 0.11215\n",
            "warning: standard deviation set to be too high\n",
            "minimum distance: 0.021819783374667168, noise level: 0.023583539575338364\n",
            "Training acc: 1.0 at epoch 160, now testing...\n",
            "Average Meta-Test Accuracy: 0.70227, Meta-Test Accuracy Std: 0.10956\n",
            "Training acc: 0.8666666746139526 at epoch 161, now testing...\n",
            "Average Meta-Test Accuracy: 0.71773, Meta-Test Accuracy Std: 0.11086\n",
            "warning: standard deviation set to be too high\n",
            "minimum distance: 0.024960799142718315, noise level: 0.023583539575338364\n",
            "warning: standard deviation set to be too high\n",
            "minimum distance: 0.022918518632650375, noise level: 0.023583539575338364\n",
            "warning: standard deviation set to be too high\n",
            "minimum distance: 0.024150041863322258, noise level: 0.023583539575338364\n",
            "warning: standard deviation set to be too high\n",
            "minimum distance: 0.023983098566532135, noise level: 0.023583539575338364\n",
            "warning: standard deviation set to be too high\n",
            "minimum distance: 0.019604044035077095, noise level: 0.023583539575338364\n",
            "Training acc: 0.8666666746139526 at epoch 162, now testing...\n",
            "Average Meta-Test Accuracy: 0.70993, Meta-Test Accuracy Std: 0.10856\n",
            "Training acc: 0.8666666746139526 at epoch 163, now testing...\n",
            "Average Meta-Test Accuracy: 0.70107, Meta-Test Accuracy Std: 0.10909\n",
            "Training acc: 1.0 at epoch 164, now testing...\n",
            "Average Meta-Test Accuracy: 0.67547, Meta-Test Accuracy Std: 0.11856\n",
            "Training acc: 0.800000011920929 at epoch 165, now testing...\n",
            "Average Meta-Test Accuracy: 0.68380, Meta-Test Accuracy Std: 0.11264\n",
            "Training acc: 0.9333333373069763 at epoch 166, now testing...\n",
            "Average Meta-Test Accuracy: 0.68973, Meta-Test Accuracy Std: 0.11776\n",
            "Training acc: 0.5333333611488342 at epoch 167, now testing...\n",
            "Average Meta-Test Accuracy: 0.68373, Meta-Test Accuracy Std: 0.11708\n",
            "Training acc: 1.0 at epoch 168, now testing...\n",
            "Average Meta-Test Accuracy: 0.67020, Meta-Test Accuracy Std: 0.11164\n",
            "Training acc: 0.9333333373069763 at epoch 169, now testing...\n",
            "Average Meta-Test Accuracy: 0.66940, Meta-Test Accuracy Std: 0.10994\n",
            "Training acc: 0.6666666865348816 at epoch 170, now testing...\n",
            "Average Meta-Test Accuracy: 0.65480, Meta-Test Accuracy Std: 0.10359\n",
            "Training acc: 1.0 at epoch 171, now testing...\n",
            "Average Meta-Test Accuracy: 0.67067, Meta-Test Accuracy Std: 0.10542\n",
            "Training acc: 0.800000011920929 at epoch 172, now testing...\n",
            "Average Meta-Test Accuracy: 0.68880, Meta-Test Accuracy Std: 0.10293\n",
            "Training acc: 0.8666666746139526 at epoch 173, now testing...\n",
            "Average Meta-Test Accuracy: 0.68373, Meta-Test Accuracy Std: 0.10090\n",
            "Training acc: 0.6000000238418579 at epoch 174, now testing...\n",
            "Average Meta-Test Accuracy: 0.69660, Meta-Test Accuracy Std: 0.10690\n",
            "Training acc: 1.0 at epoch 175, now testing...\n",
            "Average Meta-Test Accuracy: 0.71387, Meta-Test Accuracy Std: 0.10160\n",
            "Training acc: 0.9333333373069763 at epoch 176, now testing...\n",
            "Average Meta-Test Accuracy: 0.73867, Meta-Test Accuracy Std: 0.10353\n",
            "Training acc: 1.0 at epoch 177, now testing...\n",
            "Average Meta-Test Accuracy: 0.74740, Meta-Test Accuracy Std: 0.09898\n",
            "Training acc: 0.6000000238418579 at epoch 178, now testing...\n",
            "Average Meta-Test Accuracy: 0.70220, Meta-Test Accuracy Std: 0.10407\n",
            "Training acc: 0.9333333373069763 at epoch 179, now testing...\n",
            "Average Meta-Test Accuracy: 0.66133, Meta-Test Accuracy Std: 0.11589\n",
            "Training acc: 0.8666666746139526 at epoch 180, now testing...\n",
            "Average Meta-Test Accuracy: 0.66107, Meta-Test Accuracy Std: 0.11288\n",
            "Training acc: 1.0 at epoch 181, now testing...\n",
            "Average Meta-Test Accuracy: 0.66113, Meta-Test Accuracy Std: 0.10975\n",
            "Training acc: 0.7333333492279053 at epoch 182, now testing...\n",
            "Average Meta-Test Accuracy: 0.70780, Meta-Test Accuracy Std: 0.10898\n",
            "Training acc: 0.7333333492279053 at epoch 183, now testing...\n",
            "Average Meta-Test Accuracy: 0.73700, Meta-Test Accuracy Std: 0.10675\n",
            "Training acc: 0.800000011920929 at epoch 184, now testing...\n",
            "Average Meta-Test Accuracy: 0.74533, Meta-Test Accuracy Std: 0.10464\n",
            "Training acc: 1.0 at epoch 185, now testing...\n",
            "Average Meta-Test Accuracy: 0.71580, Meta-Test Accuracy Std: 0.10545\n",
            "Training acc: 1.0 at epoch 186, now testing...\n",
            "Average Meta-Test Accuracy: 0.74240, Meta-Test Accuracy Std: 0.09914\n",
            "Training acc: 1.0 at epoch 187, now testing...\n",
            "Average Meta-Test Accuracy: 0.74647, Meta-Test Accuracy Std: 0.10023\n",
            "Training acc: 0.7333333492279053 at epoch 188, now testing...\n",
            "Average Meta-Test Accuracy: 0.74147, Meta-Test Accuracy Std: 0.10339\n",
            "warning: standard deviation set to be too high\n",
            "minimum distance: 0.02142513170838356, noise level: 0.023583539575338364\n",
            "Training acc: 0.7333333492279053 at epoch 189, now testing...\n",
            "Average Meta-Test Accuracy: 0.70533, Meta-Test Accuracy Std: 0.10962\n",
            "Training acc: 0.9333333373069763 at epoch 190, now testing...\n",
            "Average Meta-Test Accuracy: 0.73240, Meta-Test Accuracy Std: 0.10323\n",
            "Training acc: 0.8666666746139526 at epoch 191, now testing...\n",
            "Average Meta-Test Accuracy: 0.69727, Meta-Test Accuracy Std: 0.11132\n",
            "Training acc: 0.7333333492279053 at epoch 192, now testing...\n",
            "Average Meta-Test Accuracy: 0.64320, Meta-Test Accuracy Std: 0.11780\n",
            "Training acc: 0.9333333373069763 at epoch 193, now testing...\n",
            "Average Meta-Test Accuracy: 0.60920, Meta-Test Accuracy Std: 0.11413\n",
            "Training acc: 0.8666666746139526 at epoch 194, now testing...\n",
            "Average Meta-Test Accuracy: 0.72287, Meta-Test Accuracy Std: 0.11232\n",
            "Training acc: 0.800000011920929 at epoch 195, now testing...\n",
            "Average Meta-Test Accuracy: 0.72460, Meta-Test Accuracy Std: 0.10786\n",
            "Training acc: 0.800000011920929 at epoch 196, now testing...\n",
            "Average Meta-Test Accuracy: 0.71720, Meta-Test Accuracy Std: 0.11102\n",
            "Training acc: 0.7333333492279053 at epoch 197, now testing...\n",
            "Average Meta-Test Accuracy: 0.73400, Meta-Test Accuracy Std: 0.10975\n",
            "Training acc: 0.9333333373069763 at epoch 198, now testing...\n",
            "Average Meta-Test Accuracy: 0.72367, Meta-Test Accuracy Std: 0.10844\n",
            "Training acc: 0.2666666805744171 at epoch 199, now testing...\n",
            "Average Meta-Test Accuracy: 0.71593, Meta-Test Accuracy Std: 0.11351\n",
            "[0.54546666, 0.5903334, 0.65333337, 0.74366677, 0.64573336, 0.65860003, 0.6755334, 0.7112667, 0.7448668, 0.75866675, 0.7498668, 0.7276001, 0.72620004, 0.5742, 0.6135334, 0.66780007, 0.7261334, 0.7508001, 0.72306675, 0.7397334, 0.7178668, 0.74140006, 0.7350668, 0.72206676, 0.6794, 0.6727334, 0.6599334, 0.6513334, 0.62893337, 0.6542001, 0.6362667, 0.69146675, 0.7242667, 0.681, 0.6862667, 0.6833334, 0.6686, 0.6696, 0.68680006, 0.7117334, 0.6893334, 0.69553345, 0.64266676, 0.6727334, 0.6751334, 0.7358668, 0.7498667, 0.7838001, 0.7168001, 0.7029334, 0.67446667, 0.65413344, 0.63180006, 0.64580005, 0.6544667, 0.6979335, 0.7228001, 0.6706001, 0.67980003, 0.72266674, 0.7477334, 0.69613343, 0.6954, 0.63753337, 0.75966674, 0.7864001, 0.8026667, 0.7647334, 0.7427334, 0.75006676, 0.7081334, 0.7846001, 0.8120668, 0.79280007, 0.7124667, 0.7264667, 0.7311334, 0.7080667, 0.6970001, 0.7317334, 0.6989334, 0.6718667, 0.72593343, 0.6959334, 0.703, 0.7038667, 0.6783334, 0.6812667, 0.65313333, 0.63866675, 0.64826673, 0.6964001, 0.7056001, 0.7100001, 0.7409334, 0.6882, 0.6837334, 0.69140005, 0.6916667, 0.67466676, 0.69246674, 0.7151334, 0.7234667, 0.73926675, 0.73213345, 0.7070001, 0.7042001, 0.7380668, 0.74240017, 0.7396001, 0.71913344, 0.69813335, 0.71880007, 0.7406001, 0.7423334, 0.7437334, 0.75633335, 0.7085334, 0.71953344, 0.72226673, 0.6737334, 0.66560006, 0.6857334, 0.67600006, 0.6750667, 0.7025334, 0.7417334, 0.75046676, 0.7264, 0.7066, 0.6848, 0.6727334, 0.65480006, 0.6774001, 0.6852001, 0.7052667, 0.7368668, 0.7019334, 0.68066674, 0.6709334, 0.6536667, 0.69366676, 0.6998668, 0.71266675, 0.6862667, 0.6544667, 0.6842, 0.69040006, 0.68880004, 0.69580007, 0.6894001, 0.7198667, 0.76300013, 0.7690001, 0.76646674, 0.69746673, 0.6994, 0.7067334, 0.6949334, 0.7006001, 0.70226675, 0.7177334, 0.70993334, 0.7010668, 0.67546666, 0.68380004, 0.6897334, 0.6837334, 0.67020005, 0.66940004, 0.65480006, 0.67066675, 0.68880004, 0.6837334, 0.6966001, 0.7138668, 0.7386668, 0.74740016, 0.70220006, 0.6613334, 0.6610667, 0.6611334, 0.7078001, 0.73700005, 0.7453334, 0.7158001, 0.7424001, 0.7464668, 0.74146676, 0.70533335, 0.73240006, 0.6972667, 0.6432001, 0.60920006, 0.7228667, 0.7246001, 0.7172001, 0.73400015, 0.7236667, 0.7159333]\n",
            "[0.46666664, 0.59333336, 0.50666666, 0.5466667, 0.6066667, 0.64, 0.5866667, 0.6533333, 0.62666667, 0.64, 0.64, 0.65999997, 0.7866667, 0.68, 0.68, 0.61333334, 0.76000005, 0.7, 0.8133334, 0.71333337, 0.7266667, 0.7333334, 0.68666667, 0.79333335, 0.7266667, 0.82, 0.81333333, 0.7666667, 0.7200001, 0.7266667, 0.73999995, 0.73333335, 0.78000003, 0.84000003, 0.8066667, 0.74, 0.73333335, 0.88, 0.8066667, 0.71333337, 0.70666665, 0.78666663, 0.84, 0.73333335, 0.87333333, 0.8533333, 0.79333335, 0.7466666, 0.8000001, 0.8, 0.79333335, 0.8533333, 0.86, 0.84666663, 0.8, 0.8333334, 0.78666663, 0.8066667, 0.82000005, 0.81333333, 0.74666667, 0.7666667, 0.8266667, 0.84666663, 0.82, 0.9, 0.8, 0.8333334, 0.8666667, 0.8, 0.88666666, 0.8533333, 0.88666666, 0.8266667, 0.8666667, 0.78666663, 0.88, 0.86, 0.9, 0.7266667, 0.8133334, 0.8666667, 0.9, 0.88666666, 0.87333333, 0.82, 0.90666664, 0.8333333, 0.9, 0.8533333, 0.88, 0.87333333, 0.9, 0.87333333, 0.8333334, 0.9266666, 0.86, 0.75333333, 0.86, 0.8933333, 0.94000006, 0.87333333, 0.88666666, 0.90666676, 0.84666663, 0.9266666, 0.8666667, 0.81333333, 0.88666666, 0.7933334, 0.87333333, 0.8, 0.88, 0.9, 0.87333333, 0.85333335, 0.84000003, 0.88, 0.9333334, 0.9333334, 0.8933333, 0.90666676, 0.82666665, 0.9533334, 0.8533333, 0.84666663, 0.82, 0.90666664, 0.8866668, 0.8066667, 0.82666665, 0.8333334, 0.84, 0.8666667, 0.85333335, 0.8666667, 0.90666664, 0.88666666, 0.9266666, 0.8133334, 0.82000005, 0.76, 0.90666664, 0.9533334, 0.8666667, 0.81333333, 0.8666667, 0.93999994, 0.9133333, 0.88666666, 0.90666664, 0.8333334, 0.8333334, 0.9133333, 0.81333333, 0.87333333, 0.9266666, 0.7733334, 0.84, 0.91999996, 0.8466667, 0.9133333, 0.9, 0.9, 0.8466667, 0.86, 0.90666664, 0.85333335, 0.90666664, 0.84000003, 0.88666666, 0.93999994, 0.9000001, 0.88666666, 0.8, 0.93999994, 0.8666667, 0.9333333, 0.86, 0.88666666, 0.88, 0.9333333, 0.8466667, 0.8333333, 0.93999994, 0.86, 0.98, 0.8666667, 0.85333335, 0.9200001, 0.9133333, 0.8933333, 0.87333333, 0.8933333, 0.88666666, 0.91999996, 0.8066667, 0.90666664, 0.88, 0.8333333]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_b_UrLK6ZuU"
      },
      "source": [
        "np.save(\"test\", test_acc)\n",
        "np.save(\"train\", train_acc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}